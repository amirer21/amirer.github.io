---
title: Artificial Intelligence - Summary of Introductory Deep Learning Concepts
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
tags:
- AI
toc: true
toc_sticky: true
toc_label: 목차
description: Essential Concepts for Beginners in Deep Learning (From Activation Functions to Transfer Learning)
article_tag1: AI
article_tag2: 
article_tag3: 
article_section: 
meta_keywords: AI, LLM, Langchain
last_modified_at: '2024-08-23 21:00:00 +0800'
---

### 1. **Basics of Deep Learning: Essential Library Numpy**
To efficiently perform basic operations in deep learning, especially vector and matrix calculations, understanding `Numpy` is essential. Numpy is the most widely used library for numerical computations in machine learning and deep learning, enabling fast and efficient calculations. It can be used for various tasks, from data preprocessing to model training.

### 2. **Activation Functions: The Heart of Deep Learning Models**
One of the most crucial elements in deep learning models is the activation function. An activation function converts input signals into output signals, adding non-linearity to neural networks.

- The most commonly used activation function, `ReLU (Rectified Linear Unit)`, is particularly effective at solving the vanishing gradient problem.
- Other functions, such as `Hyperbolic Tangent` and `Sigmoid`, are still useful in certain situations.

### 3. **Deep Learning Architectures: Understanding CNN and DNN**
Various architectures have been developed in deep learning to solve different problems.
- `CNN (Convolutional Neural Network)`, specialized for image processing, has achieved great success through models like LeNet, AlexNet, and VGG.
- In particular, `R-CNN` demonstrates powerful performance in object detection. Additionally, `Deep Neural Networks (DNN)` are extended models of multi-layer perceptrons, forming the core of deep learning.

### 4. **Transfer Learning: Learning with Less Data**
Transfer learning is a method of using a pretrained model to perform new tasks. This approach enables effective learning even without large datasets. The primary benefit of transfer learning is that it allows **"learning with less data."** This is particularly useful in environments with limited data and can be easily applied to new tasks.

### 5. **Model Performance Evaluation and Hyperparameter Tuning**
Various metrics are used to evaluate the performance of a model. Accuracy, recall, and precision are representative performance evaluation metrics. Additionally, hyperparameter tuning plays a critical role in optimizing model performance, with methods like grid search, random search, and Bayesian optimization commonly used. It is also important to consider the `vanishing gradient problem` during this process. However, the `number of parameters` is more of a measure of model complexity rather than performance evaluation.

### 6. **Generative Models: Use of GAN and VAE**
`GAN (Generative Adversarial Network)` and `VAE (Variational Autoencoder)` are generative models for data generation and compression.
- GANs excel at generating high-quality data through the competition between the generator and discriminator.
- VAEs are useful for generating and compressing data through latent space.

### 7. **Purpose and Characteristics of Specific Models**

- `BERT` is a model that demonstrates powerful performance in natural language processing tasks, efficiently performing tasks like sentence classification through bidirectional learning and pretraining. BERT is **not primarily used for "image generation,"** nor is it suitable for real-time data processing. It is characterized by utilizing pretrained models.

- Additionally, `RAG (Relevance-Augmented Generation)` is gaining attention as a multimodal learning model capable of integrating and processing various types of data.

### 8. **Data Augmentation and Processing Techniques**
Data augmentation is a crucial technique for improving a model's generalization performance. For image data, techniques such as rotation, cropping, and flipping are used, providing diverse examples for model training by transforming the original data. **"Rearrangement"** is not typically considered data transformation but can be used as a method to alter the structure of data.

### Conclusion:
Deep learning is a field intertwined with complex concepts and techniques, but understanding the core concepts and applying them in practice is the first step toward successful model development.