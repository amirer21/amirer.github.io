---
title: 인공지능 -  NLP (4) NLP 처리를 위한 인공지능 모델 비교 - 딥러닝과 Transformer의 활용
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
tags:
- AI
- NLP
- DeepLearning
toc: true
toc_sticky: true
toc_label: 목차
description: 이전에 정리한 NLP 관련 예제 3개 내용을 정리한 글입니다.
article_tag1: AI
article_tag2: NLP
article_tag3: DeepLearning
article_section: 
meta_keywords: AI, OpenAI, LangChain
last_modified_at: '2024-08-31 21:00:00 +0800'
---

인공지능에서 언어를 처리하는 과정은 일반적으로 데이터 수집, 전처리, 임베딩, 모델링, 학습, 예측 등의 단계로 나눌 수 있습니다. 이 과정을 기준으로 이전 과정을 비교하여 정리하면 다음과 같습니다

### 1. **데이터 수집 및 준비**

- **첫 번째 코드 (`Deep-Learning-for-NLP-Tasks`)**
  - **데이터 준비**: 간단한 예제 데이터(문장과 감정 레이블)를 사용합니다.
  - **레이블 인코딩**: `LabelEncoder`를 사용하여 텍스트 레이블을 숫자로 변환합니다.
  
- **두 번째 코드 (`Neural-Network-for-Text-Classification`)**
  - **데이터 준비**: 동일하게 텍스트와 레이블로 구성된 간단한 데이터셋을 사용합니다.
  - **레이블 인코딩**: 레이블은 이미 숫자로 제공되므로 추가적인 인코딩 과정은 필요 없습니다.
  
- **세 번째 코드 (`Text-Generation-with-Pretrained-Transformer-Models`)**
  - **데이터 준비**: 단일 문장을 예측 대상으로 사용합니다.
  - **레이블 인코딩**: 레이블이 필요하지 않으며 모델은 미리 학습된 상태에서 인퍼런스만 수행합니다.

### 2. **전처리**

- **첫 번째 코드**
  - **토크나이징**: `AutoTokenizer`를 사용하여 텍스트를 BERT가 이해할 수 있는 형식으로 토크나이징합니다.
  - **패딩 및 자르기**: 최대 길이를 설정하여 토큰 시퀀스를 일정 길이로 맞춥니다.

- **두 번째 코드**
  - **토크나이징**: `Tokenizer`를 사용하여 텍스트를 숫자 시퀀스로 변환합니다.
  - **패딩**: `pad_sequences`로 시퀀스 길이를 맞춥니다. 이 과정에서 단순한 단어 인덱스 기반의 시퀀스가 생성됩니다.

- **세 번째 코드**
  - **토크나이징**: `AutoTokenizer`로 입력 텍스트를 토크나이징하고, BERT 형식에 맞게 변환합니다.
  - **패딩 및 자르기**: 첫 번째 코드와 유사하게 최대 길이로 자르고 패딩합니다.

### 3. **임베딩 및 피처 추출**

- **첫 번째 코드**
  - **임베딩**: 사전 학습된 BERT 모델의 임베딩 레이어를 사용하여 텍스트의 문맥적 의미를 벡터화합니다.
  
- **두 번째 코드**
  - **임베딩**: `Embedding` 레이어를 사용하여 단어를 16차원 벡터로 임베딩합니다. 이 과정은 임베딩 레이어의 가중치를 학습하면서 진행됩니다.
  
- **세 번째 코드**
  - **임베딩**: BERT의 사전 학습된 임베딩을 사용합니다. 이는 모델의 인퍼런스 과정에서 자동으로 수행됩니다.

### 4. **모델링**

- **첫 번째 코드**
  - **모델**: BERT 기반의 시퀀스 분류 모델을 사용하며, 미세 조정(Fine-tuning)을 통해 감정 분류를 수행할 수 있도록 조정합니다.
  
- **두 번째 코드**
  - **모델**: 간단한 신경망 구조로 임베딩 -> GlobalAveragePooling1D -> Dense 레이어로 구성된 순차적 신경망입니다.
  
- **세 번째 코드**
  - **모델**: BERT 모델을 그대로 사용하며 학습 없이 인퍼런스만 수행합니다.

### 5. **학습**

- **첫 번째 코드**
  - **학습 과정**: 학습 데이터와 검증 데이터를 사용해 BERT 모델을 3 에포크 동안 학습시킵니다.
  
- **두 번째 코드**
  - **학습 과정**: 신경망 모델을 10 에포크 동안 학습합니다. 학습이 비교적 빠르고 단순한 구조입니다.
  
- **세 번째 코드**
  - **학습 과정**: 학습이 없습니다. 사전 학습된 모델을 사용하여 예측만 수행합니다.

### 6. **예측 및 결과 출력**

- **첫 번째 코드**
  - **예측**: 입력된 텍스트에 대해 감정을 예측하고 결과를 긍정/부정으로 출력합니다.
  
- **두 번째 코드**
  - **예측**: 새로운 텍스트를 입력하여 예측하고 0.5를 기준으로 긍정/부정을 출력합니다.
  
- **세 번째 코드**
  - **예측**: 입력된 문장의 감정을 예측하여 긍정/부정을 출력합니다. 빠르게 인퍼런스 결과를 제공합니다.

### 종합 정리

- **첫 번째 코드 (`Deep-Learning-for-NLP-Tasks`)**는 딥러닝 모델, 특히 사전 학습된 BERT를 활용해 NLP 작업에 적용하는 과정을 보여줍니다.
- **두 번째 코드 (`Neural-Network-for-Text-Classification`)**는 간단한 신경망 모델을 사용하여 텍스트 분류 작업을 수행하는 과정을 설명합니다.
- **세 번째 코드 (`Text-Generation-with-Pretrained-Transformer-Models`)**는 사전 학습된 Transformer 모델을 사용해 텍스트의 감정을 빠르게 예측하는 예시로, 학습 없이 인퍼런스만 수행합니다.

이 정리에서 각 코드는 인공지능의 NLP 처리 과정에서 데이터 준비부터 예측까지의 단계를 다루며, 각각의 접근 방식과 모델의 복잡도에 따른 차이를 명확히 보여줍니다.