---
title: 인공지능 - Hidden State Update Equation
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
tags:
- AI
- RNN
toc: true
toc_sticky: true
toc_label: 목차
description: 인공지능 - Hidden State Update Equation
article_tag1: AI
article_tag2: RNN
article_tag3: 
article_section: 
meta_keywords: AI, RNN
last_modified_at: '2024-05-29 21:00:00 +0800'
---

# RNN의 은닉 상태 업데이트 공식

## 수식
$$ h_t = \tanh(h_{t-1}W_h + x_tW_x + b) $$

### 형상(Shape) 확인
- $h_{t-1}$의 shape: $(N, H)$
- $W_h$의 shape: $(H, H)$
- $x_t$의 shape: $(N, D)$
- $W_x$의 shape: $(D, H)$
- $b$의 shape: $(H,)$ 또는 $(1, H)$
- $h_t$의 shape: $(N, H)$

여기서:
- $N$은 미니배치 크기, 즉 동시에 처리하는 입력 데이터의 개수입니다.
- $D$는 입력 벡터의 차원 수입니다.
- $H$는 은닉 상태 벡터의 차원 수입니다.

## 과정 설명
1. **이전 은닉 상태와 은닉 상태 가중치 곱셈**:
   - $h_{t-1}$의 shape는 $(N, H)$
   - $W_h$의 shape는 $(H, H)$
   - $h_{t-1}W_h$의 결과 shape는 $(N, H)$

2. **현재 입력과 입력 가중치 곱셈**:
   - $x_t$의 shape는 $(N, D)$
   - $W_x$의 shape는 $(D, H)$
   - $x_tW_x$의 결과 shape는 $(N, H)$

3. **합산 및 비선형 활성화 함수 적용**:
   - $h_{t-1}W_h$와 $x_tW_x$는 둘 다 shape가 $(N, H)$이므로 두 행렬을 더할 수 있습니다.
   - $b$의 shape는 $(H,)$이지만 브로드캐스팅되어 $(N, H)$로 적용됩니다.
   - 최종적으로 $\tanh$ 함수를 통해 비선형 활성화가 적용되어 $h_t$의 shape가 $(N, H)$가 됩니다.

## 은닉 상태란 무엇인가?
은닉 상태(hidden state)란 RNN(Recurrent Neural Network)에서 이전 시점의 정보를 현재 시점으로 전달하는 메모리 역할을 하는 벡터입니다. 이는 시퀀스 데이터를 처리할 때, 이전 입력의 영향을 반영하여 현재 출력에 중요한 정보를 제공합니다. 은닉 상태는 매 타임스텝마다 갱신되며, 네트워크가 과거의 정보를 기억하고 이를 바탕으로 학습할 수 있게 합니다. RNN에서는 주로 $ h_t $로 표기됩니다.

## tanh 함수란 무엇인가?
tanh(하이퍼볼릭 탄젠트) 함수는 비선형 활성화 함수로, 입력 값을 -1에서 1 사이의 값으로 변환합니다. 수식은 다음과 같습니다:

$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

RNN에서 tanh 함수는 은닉 상태 값을 비선형적으로 변환하여, 값이 너무 커지거나 작아지는 것을 방지하고, 학습의 안정성을 높입니다. 이로 인해, 네트워크가 더 복잡한 패턴을 학습할 수 있게 도와줍니다.

예를 들어, 입력 값이 2일 때, tanh(2)는 약 0.964가 됩니다. 반대로, 입력 값이 -2일 때, tanh(-2)는 약 -0.964가 됩니다. 이는 입력 값을 -1에서 1 사이로 제한하여, 신경망의 출력 값이 과도하게 커지지 않도록 합니다.

## 비선형적이란 무엇인가?
비선형적이란 입력과 출력의 관계가 단순한 직선이 아닌 경우를 말합니다. 비선형 함수는 곡선 형태를 가지며, 입력 값의 변화에 따라 출력 값이 비례하지 않습니다. 예를 들어, 직선 그래프는 선형적이지만, 곡선 그래프는 비선형적입니다. 신경망에서 비선형성을 도입하면 복잡한 패턴과 관계를 학습할 수 있게 되어, 더 정교한 예측과 분류가 가능합니다.

## 이 공식의 명칭
이 공식은 순환 신경망(Recurrent Neural Network, RNN)에서 사용되는 은닉 상태 업데이트 식입니다. 이 식은 현재 시점의 은닉 상태 $h_t$를 이전 시점의 은닉 상태 $h_{t-1}$, 현재 입력 $x_t$, 가중치 행렬 $W_h$와 $W_x$, 그리고 바이어스 $b$를 사용하여 계산합니다. 여기서 $tanh$ 함수는 비선형 활성화 함수로, 은닉 상태 값이 -1에서 1 사이로 제한되도록 합니다. 이 공식은 RNN이 시퀀스 데이터에서 시간 의존성을 학습하는 데 핵심적인 역할을 합니다.

## RNN에서 repeat이란?
RNN에서 "repeat"는 동일한 네트워크 구조와 가중치를 여러 타임스텝에 걸쳐 반복적으로 적용하는 것을 의미합니다. 이는 시퀀스 데이터의 각 시점에서 입력을 처리하고, 이전 시점의 은닉 상태를 다음 시점으로 전달하여 학습합니다. 이를 통해 시간 의존성을 학습하고 정보를 전달합니다.
